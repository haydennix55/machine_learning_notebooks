{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vectore Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[50 Points] Problem 1 - Basic concepts of SVM\n",
    "---\n",
    "\n",
    "**Part A**: \n",
    "* What are the main differences between the primal and the dual representations?\n",
    "\n",
    "* For $\\xi$, $C$, $\\alpha$, and $\\beta$, what is their role and if there is a special value what is the value and what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "17ad2a9bbee546deccd82b3da4d9093e",
     "grade": true,
     "grade_id": "cell-f177349aed9aabcf",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**In the primal problem, we are minimizing the objective function with regard to $w, b, \\xi$. In this representaion, we solve directly for $w$ and $b$.**\n",
    "\n",
    "**The dual problem is a different perspective on the primal in which take the langrangian representation of the primal problem and turn in from a min-max problem to a max-min problem, which is allowed under strong duality. In the dual problem, we are maximixing a different objective function with regard to $\\alpha$. Notice, this dual objective function does not have $w$ or $b$ at all. Instead, we find the maximizing $\\alpha$ value and use the KKT conditions to determine $w$ and $b$.**\n",
    "\n",
    "**$\\xi$: slack variable, how \"wrong\" a classifed point is. Special values:**\n",
    "\n",
    "- $\\xi_i$ = 0 when i is at least 1 margin distance from the decision boundary on the correct side. \n",
    "- $\\xi_i$ = 2 when i is 1 margin distance from the decision boundary on the incorrect side.\n",
    "\n",
    "**$C$: the tradeoff between the margin and slack variable terms. It essentially means \"how much should misclassified points affect the model\". Special values:**\n",
    "\n",
    "- $C$ = $\\infty$ means there is an infinite penalty for missclassified points, which essentially turns soft-margin SVM into hard-margin\n",
    "- $C$ = 0 is not exactly a \"special value\", but it would mean that you don't care at all if points are misclassified, which defeats the point of a classifier\n",
    "\n",
    "**$\\alpha$: a KKT multiplier that, in the primal lagrangian, enforces the $y_i(w^Tx_i + b) - 1 + \\xi_i \\geq 0$ primal condition and therefore, in the converse dual problem, the value when want to maximize with. Special values:**\n",
    "\n",
    "It helped me to think of $\\alpha$ in the lagrangian formulation as an adversarial term. If our condition above was not met (i.e $y_i(w^Tx_i + b) - 1 + \\xi_i < 0$), $\\alpha$ could equal infinity, making this term huge when we should be minimizing. Conversely, if the condition above is met, $\\alpha$ equal to 0 would be the best adversarial option.\n",
    "\n",
    "**$\\beta$: a KKT multiplier that, in the primal lagrangian, enforces the $\\xi_i \\geq 0$ primal condition, similar to the $\\alpha$ above**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B**: \n",
    "\n",
    " * Given a weight vector, implement the *find_support* function that returns the indices of the support vectors.\n",
    " * Given a weight vector, implement the *find_slack* function that returns the indices of the vectors with nonzero slack.\n",
    " * Given the alpha dual vector, implement the *weight_vector* function that returns the corresponding weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f9819411bf28b6114759d3be6ef4d20f",
     "grade": false,
     "grade_id": "cell-14c104d96c00c2eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "kINSP = np.array([(1, 8, +1),\n",
    "               (7, 2, -1),\n",
    "               (6, -1, -1),\n",
    "               (-5, 0, +1),\n",
    "               (-5, 1, -1),\n",
    "               (-5, 2, +1),\n",
    "               (6, 3, +1),\n",
    "               (6, 1, -1),\n",
    "               (5, 2, -1)])\n",
    "\n",
    "kSEP = np.array([(-2, 2, +1),    # 0 - A\n",
    "              (0, 4, +1),     # 1 - B\n",
    "              (2, 1, +1),     # 2 - C\n",
    "              (-2, -3, -1),   # 3 - D\n",
    "              (0, -1, -1),    # 4 - E\n",
    "              (2, -3, -1),    # 5 - F\n",
    "              ])\n",
    "\n",
    "\n",
    "def weight_vector(x, y, alpha):\n",
    "    \"\"\"\n",
    "    Given a vector of alphas, compute the primal weight vector w.\n",
    "    The vector w should be returned as an Numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros(len(x[0]))\n",
    "    for i in range(len(x)):\n",
    "        w += x[i] * y[i] * alpha[i]\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "def find_support(x, y, w, b, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    Given a set of training examples and primal weights, return the indices\n",
    "    of all of the support vectors as a set.\n",
    "    \"\"\"\n",
    "    support = set([i for i in range(len(x)) if abs(y[i]*(np.dot(w,x[i]) + b) - 1) <= tolerance])\n",
    "    return support\n",
    "\n",
    "\n",
    "\n",
    "def find_slack(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Given a set of training examples and primal weights, return the indices\n",
    "    of all examples with nonzero slack as a set.\n",
    "    \"\"\"\n",
    "\n",
    "    slack = set([i for i in range(len(x)) if (y[i] * (np.dot(w, x[i]) + b)) < 0])\n",
    "    return slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac70e6c01a6d463c705fce3657daeb77",
     "grade": true,
     "grade_id": "cell-3c7d7f432578009e",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART C**\n",
    "\n",
    "The goal of this problem is to correctly classify test data points, given a training data set.\n",
    "For this problem, assume that we are training an SVM with a quadratic kernelâ€“ that is, our kernel function is a polynomial kernel of degree 2. You are given the data set presented in Figure 1. The slack penalty C will determine the location of the decision boundary.\n",
    "\n",
    "Justify the following questions in a sentence or via drawing decision boundary.\n",
    "![training_data](./data/data.png)\n",
    "\n",
    "* Where would the decision boundary be for very large values of C ?\n",
    "* Where you would expect the decision boundary to be if  C = 0 ?\n",
    "* Which of the two cases above would you expect to generalize better on test data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "14587983929cbe06427a6f5fc256be7d",
     "grade": true,
     "grade_id": "cell-02406ba497be1623",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**1. I would expect the boundary for very large values of C to be a parabola (due to degree 2) that wraps around the green points. This is because the high slack penalty means the model will shift the decision boundary to avoid a high penalty for misclassifiying the two outlying red points, even at the cost of reducing the margin (high variance, low bias)**\n",
    "\n",
    "**2. I woulc expect the boundary for C = 0 (or in reality it would likely be very close to 0) to pass through the large gap between the majority red and green clusters, and thus misclassifying the two red outlying red points. This is because a low slack penalty means the model will care more about maximizing the margin than missclassifying some points (high bias, low variance)**\n",
    "\n",
    "**3. In this example, I would expect the low C to generalize better on test data. A very low C will roughly classify data in the bottom left corner as red and the top right corner  as green, even though a few outliers could be misclassified. A high C will overfit to the training data, and as a result would assume only points in the very upper right corner of this plot are green, likely misclassifying more points in the test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[50 points] Problem 2 -- SVM with Sklearn\n",
    "---\n",
    "\n",
    "In this problem, you are going to get familiar with important practical functions in scikit-learn such as pipeline, grid search, and cross validation. You will experiment with these using support vector machines.\n",
    "\n",
    "Note that grid search can take some time on your laptop, so make sure that your code is correct with a small subset of the training data and search a reasonable number of options.\n",
    "\n",
    "* Use the Sklearn implementation of support vector machines to train a classifier to distinguish Positive and negative sentiments\n",
    "* Experiment with linear, polynomial, and RBF kernels. In each case, perform a GridSearch to help determine optimal hyperparameters for the given model (e.g. C for linear kernel, C and p for polynomial kernel, and C and  for RBF). Comment on the experiments you ran and optimal hyperparameters you found.\n",
    "Hint: http://scikit-learn.org/stable/modules/grid_search.html\n",
    "* Comment on classification performance for each model for optimal parameters by testing on a hold-out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a dataset containing reviews and sentiments associated with it.\n",
    "\n",
    "Create a SVM Classifier to predict positive or negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "reviews  = pd.read_csv('./data/reviews.csv')\n",
    "train, test = train_test_split(reviews, test_size=0.2, random_state=4622)\n",
    "X_train = train['reviews'].values\n",
    "X_test = test['reviews'].values\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2004, 1000, 496)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),sum(y_train),len(X_test),sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART A**\n",
    "\n",
    "Use CountVectorizer to vectorize reviews as dictionary of term frequencies.\n",
    "Define the crossvalidation split using StratifiedKFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "64899a0f9ff6f6b41457a00e1e800d57",
     "grade": true,
     "grade_id": "cell-ab58d371c35713b4",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "# CREATE CountVectorizer using sklearn.feature_extraction.text.CountVectorizer\n",
    "# Hint: use the above tokenize function\n",
    "# Hint: play with different parameters, in particular, min_df can help with generalizability\n",
    "# YOUR CODE HERE\n",
    "count_vect = CountVectorizer(tokenizer=tokenize, stop_words=en_stopwords, min_df=5)\n",
    "# split dataset using StratifiedKFold into 5 splits using sklearn.model_selection.StratifiedKFold.\n",
    "# YOUR CODE HERE\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cv = skf.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B**\n",
    "* Create pipeline with Count Vectorizer and SVM Classifier\n",
    "* Define grid search parameters\n",
    "* Create GridSearchCV object with pipeline created and fit the data.\n",
    "* Compute accuracy on best estimator from GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6e9d3128b56a233b0cfec29e0386fae1",
     "grade": true,
     "grade_id": "cell-566701928c0de790",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE GRID SEARCH PARAMETERS kernel, C, degree, gamma respectively\n",
    "# YOUR CODE HERE\n",
    "svc=SVC()\n",
    "c_range = np.logspace(-8, 3, 5, base=2)\n",
    "gamma_range = np.logspace(-5, 3, 3, base=2)\n",
    "degree_range=np.linspace(2.0,5.0, num=3)\n",
    "param_grid = dict(svc__gamma=gamma_range, svc__C=c_range, svc__kernel=('poly', 'rbf'), svc__degree=degree_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.90625000e-03 2.62780130e-02 1.76776695e-01 1.18920712e+00\n",
      " 8.00000000e+00] [0.03125 0.5     8.     ]\n"
     ]
    }
   ],
   "source": [
    "print(c_range, gamma_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2ec8874c31a93444620508a810a58bc6",
     "grade": true,
     "grade_id": "cell-a3dd5b25a9ce8feb",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 450 out of 450 | elapsed:   27.4s finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'svc__gamma': array([0.03125, 0.5    , 8.     ]), 'svc__C': array([3.90625e-03, 2.62780e-02, 1.76777e-01, 1.18921e+00, 8.00000e+00]), 'svc__kernel': ('poly', 'rbf'), 'svc__degree': array([2. , 3.5, 5. ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "# Define pipeline using make_pipeline with vectorizer and SVM Classifier\n",
    "# YOUR CODE HERE\n",
    "pipeline = make_pipeline(count_vect, svc)\n",
    "\n",
    "# Create GridSearchCV with pipeline and grid search parameters, scoring as accuracy.\n",
    "# for example grid_svm = GridSearchCV(pipeline, param_grid, cv, scoring=\"accuracy\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "grid_svm = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=cv, scoring=\"accuracy\", verbose=1)\n",
    "\n",
    "# For debugging purposes, it makes sense to use a smaller set of training set to speed up the grid search progress\n",
    "grid_svm.fit(X_train[0:50], y_train[0:50])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None,\n",
    "        stop_words=...f', max_iter=-1, probability=False, random_state=None,\n",
    "  shrinking=True, tol=0.001, verbose=False))]),\n",
    "       fit_params=None, iid='warn', n_jobs=None,\n",
    "       param_grid={'svc__gamma': array([3.125e-02, 1.000e+00, 3.200e+01]), 'svc__C': array([3.125e-02, 1.000e+00, 3.200e+01]), 'svc__kernel': ('linear', 'rbf'), 'svc__degree': array([1., 3., 5.])},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='accuracy', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:\n",
      "{'svc__C': 0.026278012976678578, 'svc__degree': 3.5, 'svc__gamma': 0.03125, 'svc__kernel': 'poly'}\n",
      "best cv score:\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "print(\"best params:\")\n",
    "print(grid_svm.best_params_)\n",
    "\n",
    "print(\"best cv score:\")\n",
    "print(grid_svm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best params:\n",
    "{'svc__C': 0.03125, 'svc__degree': 1.0, 'svc__gamma': 0.03125, 'svc__kernel': 'linear'}\n",
    "best cv score:\n",
    "0.87025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(model, X, y):\n",
    "    pred = model.predict(X)        \n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = precision_score(y, pred)\n",
    "    rec = recall_score(y, pred)\n",
    "    result = {'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.4967320261437908,\n",
       " 'acc': 0.615,\n",
       " 'precision': 0.7063197026022305,\n",
       " 'recall': 0.38306451612903225}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_results(grid_svm.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART C**\n",
    "\n",
    "Explain the overall procedure and report the final result that you obtain including which kernel and hyperparameter was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "518aa09b353c4f15f8aebb399798d88a",
     "grade": true,
     "grade_id": "cell-6fecb92ed6ad5abe",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The overall procedure is to vectorize the text in the dataset, excluding stop words and words that occur in less than 5% of the overall dataset (thank to min_df). Then we fit an SVM model using 5 fold cross-validation on the vectorized data with every combination of paramters in our parameter grid (C, gamma, degree, kernel) to determine which were the best to use.\n",
    "\n",
    "In my case the best were:\n",
    "\n",
    "* **C**: 0.021262343752724643\n",
    "\n",
    "* **Gamma**: 0.03125\n",
    "\n",
    "* **Degree**: 2\n",
    "\n",
    "* **Kernel**: polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
