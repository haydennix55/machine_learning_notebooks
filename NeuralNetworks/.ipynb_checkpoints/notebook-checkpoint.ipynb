{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "**Acknowledgment** : Chris Ketelsen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 points] Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "**Part A** : Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights and biases as applicable:\n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$\n",
    "\n",
    "ii. $~~x_1 \\texttt{ NAND } x_2$\n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c85384be6b7c21b60e806f68a49d0a2d",
     "grade": true,
     "grade_id": "cell-5504bb80827fe61b",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**i. Yes, NOT $x_1$ can be learned from a single layer perceptron by setting the weight $w_1$ to $-1$ and the bias to $1$. When $x_1 = 1$, the output will be $0$. When $x_1 = 0$, the output will be $1$**\n",
    "\n",
    "**ii. Yes. $x_1$ NAND $x_2$ can be learned from a single layer perceptron by setting both $w_1$ and $w_2$ to $-1$ and bias to $1$. If one or both of the inputs are $1$, the output will be $0$. If both the inputs are $0$, the output will be $1$**\n",
    "\n",
    "**iii No. $x_1$ XNOR $x_2$ cannot be learned from a single layer perceptron because the 4 cases are not linearly seperable. If you plot (x1, x2), you will have (0,0) and (1,1) in class 1 and (0,1) and (1,0) in class 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B** : Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. Describe your architecture and state your weight matrices and bias vectors in Markdown below. Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4ba359fcd10f0c6fd31fbe9f9985af6c",
     "grade": true,
     "grade_id": "cell-fd1e475a5ef92def",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$x_1$ XNOR $x_2$ is the same as $(x_1$ AND $x_2)$ OR $(x_1$ NAND $x_2)$.\n",
    "\n",
    "$$\n",
    "w^1 = \\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{bmatrix}, \n",
    "b^1 = \\begin{bmatrix}\n",
    "-1.5 & 0.5 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^2 = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "\\end{bmatrix}, \n",
    "b^2 = -0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6665c678216da8282104f9cf0397ceb2",
     "grade": true,
     "grade_id": "cell-330996afb5a81650",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 0] , Output: [1]\n",
      "Input: [0, 1] , Output: [0]\n",
      "Input: [1, 0] , Output: [0]\n",
      "Input: [1, 1] , Output: [1]\n"
     ]
    }
   ],
   "source": [
    "cases = [[0,0], [0,1], [1,0], [1,1]]\n",
    "weights = [[[1, 1], [-1,-1]], [[1,1]]]\n",
    "biases = [[-1.5, 0.5], [-0.5]]\n",
    "\n",
    "def compute_a_prime(case,weights,biases):\n",
    "    activations = [None] * len(biases)\n",
    "    for i in range(len(weights)):\n",
    "        dot = np.dot(case, weights[i])\n",
    "        activations[i] = 0 if dot + biases[i] <= 0 else 1\n",
    "    return activations\n",
    "\n",
    "def xnor(cases, weights, biases):\n",
    "    y_hat = []\n",
    "    for case in cases:\n",
    "        a = case\n",
    "        for i in range(len(biases)):\n",
    "            a = compute_a_prime(a, weights[i], biases[i])\n",
    "        y_hat.append(a)  \n",
    "    return y_hat\n",
    "\n",
    "y_hat = xnor(cases, weights, biases)\n",
    "for i in range(len(cases)):\n",
    "    print(\"Input:\", cases[i], \", Output:\", y_hat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 points] Problem 2 - Back propagation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ed8ae08caeac509ef436f5d18c8223c",
     "grade": true,
     "grade_id": "cell-6512c42fc5e9ce1b",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$a^1 = 0.5$\n",
    "\n",
    "$z^2 = 0.5 * 1 - 0.5 = 0$\n",
    "\n",
    "$a^2 = sigm(0) = 0.5$\n",
    "\n",
    "$z^3 = 0.5 * 1 - 0.5 = 0$\n",
    "\n",
    "$a^3 = sigm(0) = 0.5$\n",
    "\n",
    "$z^4 = 0.5 * 1 - 0.5 = 0$\n",
    "\n",
    "$a^4 = sigm(0) = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d95b2a0dc16f45336d915205d119763a",
     "grade": true,
     "grade_id": "cell-6a3b895fa888e925",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "$\\frac{\\partial C}{\\partial a^4} = a^4 - y$\n",
    "\n",
    "$g'(z) = sigm'(z) = sigm(z) * (1 - sigm(z))$\n",
    "\n",
    "$\\delta^4 = a^4 - y * sigm'(z^4) = (0.5 - 0) * sigm'(0) = 0.125$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial W^3} = \\delta^4 * a^3 = 0.125 * 0.5 = 0.0625$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^3} = \\delta^4 = 0.125$\n",
    "\n",
    "$\\delta^3 = (W^3 * \\delta^4) * sigm'(z^3) = (1.0 * 0.125) * sigm'(0) = 0.03125$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial W^2} = \\delta^3 * a^2 = 0.03125 * 0.5 = 0.015625$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^2} = \\delta^3 = 0.03125$\n",
    "\n",
    "$\\delta^2 = (W^2 * \\delta^3) * sigm'(z^2) = (1.0 * 0.03125) * sigm'(0) = 0.0078125$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial W^1 } = \\delta^2 * a^1 = 0.0078125 * 0.5 = 0.00390625$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^1} = \\delta^2 = 0.0078125$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART C** Implement following activation functions:\n",
    "* Relu\n",
    "* Sigmoid\n",
    "* softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a11601766c4f5d28f2b672ebe82b132e",
     "grade": false,
     "grade_id": "cell-3b7e3adaffe2c2ee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def relu(x):\n",
    "    return x if x > 0.0 else 0.0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def soft_max(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a065137beda1d2059a79da7236a504c",
     "grade": true,
     "grade_id": "cell-0bded3752d3226b9",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert relu(5) == 5\n",
    "assert relu(-5) == 0\n",
    "assert relu(0) == 0\n",
    "assert relu(5) == 5\n",
    "\n",
    "assert sigmoid(0.458) == 0.61253961344091512\n",
    "assert sigmoid(2) == 0.8807970779778823\n",
    "res = soft_max([1,2,4])\n",
    "temp = [0.04201007, 0.1141952 , 0.84379473]\n",
    "for i in range(len(temp)):\n",
    "    assert res[i] - temp[i] < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART D** Implement the following Loss functions:\n",
    "* mean squared error\n",
    "* mean absolute error\n",
    "* hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8c561dfa268b8b00a4ab84dbd1752d20",
     "grade": false,
     "grade_id": "cell-84456d343bfeca31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat,y):\n",
    "    return np.square(np.subtract(y, yhat)).mean()\n",
    "    \n",
    "def mean_absolute_error(yhat,y):\n",
    "    return abs(np.subtract(y, yhat)).mean()\n",
    "\n",
    "def hinge(yhat,y):\n",
    "    err = 0.0\n",
    "    for i in range(0, y.shape[0]):\n",
    "        err += max(0, 1 - y[i] * yhat[i])\n",
    "    err = err / y.shape[0]\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a390d645725dff540dceab9d23c1c9c",
     "grade": true,
     "grade_id": "cell-550e920d814cb6d3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "assert mean_squared_error(y_pred,y_true) == 0.375\n",
    "assert mean_absolute_error(y_pred,y_true) == 0.5\n",
    "assert hinge(y_pred,y_true) == 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART E** Explain the vanishing gradient problem, when would you observe this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "550c7233d3dc395aa4965b23ce1126a4",
     "grade": true,
     "grade_id": "cell-59ccf91056b5d8bb",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The vanishing gradient problem is an issue that occurs when training neural nets using gradients and back propogation. With gradient based models like stochastic graident descent, the weights and biases at a given layer are updated using the gradient (derivative) of the loss function with respect to the current weights/biases at that layer and a learning rate. The issue is that if this gradient is vanishingly small, the weight and bias updates would be vanishingly small as well, so very little \"learning\" would occur, potentially leaving the weights and biases far from optimal.\n",
    "\n",
    "The issue occurs because the the weight update gradient $\\frac{\\partial C}{\\partial W^l}$ and the bias update gradient $\\frac{\\partial C}{\\partial b^l}$ are the product of derivatives that occur later in the network (layers > $l$). If these terms are less than 1 (or at lease some of them), their product will be smaller. As a result, the earlier a layer is in a network, especially a deep network, the more terms the gradient $\\frac{\\partial C}{\\partial W^l}$ and $\\frac{\\partial C}{\\partial b^l}$ will depend on, giving them the potential to shrink to an insignifigant value. This will result in insignifigsant updates. Additionally, the update value is also based on the learning rate, which itself can be a very small number (much less than 1) and shrink the update value further.\n",
    "\n",
    "You may observe this when a network is very deep, when wieghts are initialized to Gaussian values (between 0 and 1), when using a sigmoid loss function, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 Points] Problem 3 - Build a feedForward neural network\n",
    "---\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement forward propagation, prediction, back propagation, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "\n",
    "The skeleton for the network class is below. Note that this class is almost identical to the one you worked with in the hands-On neural network in-class notebook, so you should look there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "edeb900917edc30fc748c3f4fa77237a",
     "grade": false,
     "grade_id": "cell-b8abc0ac570aac74",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(n, m) for (\n",
    "            m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        activation function\n",
    "        \"\"\"\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of activation function\n",
    "        \"\"\"\n",
    "        return sigmoid_prime(z)\n",
    "\n",
    "    def forward_prop(self, a):\n",
    "        \"\"\"\n",
    "        memory aware forward propagation for testing\n",
    "        only.  back_prop implements it's own forward_prop\n",
    "        \"\"\"\n",
    "        for l in range(self.L - 1):\n",
    "            z = np.dot(self.weights[l], a) + self.biases[l]\n",
    "            a = self.g(z)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def grad_cost(self, a, y):\n",
    "        \"\"\"\n",
    "        gradient of cost function\n",
    "        Assumes C(a,y) = (a-y)^2/2\n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "\n",
    "    def SGD_train(self, train, epochs, eta, lam=0.0, verbose=True, test=None):\n",
    "        \"\"\"\n",
    "        SGD for training parameters\n",
    "        epochs is the number of epocs to run\n",
    "        eta is the learning rate\n",
    "        lam is the regularization parameter\n",
    "        If verbose is set will print progressive accuracy updates\n",
    "        If test set is provided, routine will print accuracy on test set as learning evolves\n",
    "        \"\"\"\n",
    "        n_train = len(train)\n",
    "        epoch_accuracy_list = []  # omit\n",
    "        hidden_acc_train = 0  # omit\n",
    "        hidden_acc_test = 0  # omit\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            shuffled = np.random.permutation(n_train)\n",
    "            \n",
    "            for i in shuffled:\n",
    "                x_train = train[i][0]\n",
    "                y_train = train[i][1]\n",
    "                dW_list, db_list = self.back_prop(x_train, y_train)\n",
    "                \n",
    "                self.weights = [w - eta*(dW+lam*w) for w, dW in zip(self.weights,dW_list)]\n",
    "                self.biases = [b - eta*db for b, db in zip(self.biases, db_list)]\n",
    "                \n",
    "            if verbose:\n",
    "                if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "                    acc_train = self.evaluate(train)\n",
    "                    if test is not None:\n",
    "                        acc_test = self.evaluate(test)\n",
    "                        # epoch_accuracy_list.append((acc_test,acc_train))    #omit\n",
    "                        print(\"Epoch {:4d}: Train {:10.5f}, Test {:10.5f}\".format(\n",
    "                            epoch+1, acc_train, acc_test))\n",
    "                    else:\n",
    "                        print(\"Epoch {:4d}: Train {:10.5f}\".format(\n",
    "                            epoch+1, acc_train))\n",
    "                    \n",
    "\n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation for derivatives of C wrt parameters\n",
    "        \"\"\"\n",
    "        db_list = [np.zeros(b.shape) for b in self.biases]\n",
    "        dW_list = [np.zeros(W.shape) for W in self.weights]\n",
    "\n",
    "        a = x\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)]  # Pad with throwaway so indices match\n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(W, a) + b\n",
    "            z_list.append(z)\n",
    "            a = self.g(z)\n",
    "            a_list.append(a)\n",
    "\n",
    "        # Back propagate deltas to compute derivatives\n",
    "        # The following list gives hints on how to do it\n",
    "        # calculating delta (Error) for the output layer\n",
    "        # for the appropriate layers compute db_list[ell], dW_list[ell], delta\n",
    "\n",
    "        delta = (a_list[self.L-1] - y) * self.g_prime(z_list[self.L-1])\n",
    "        \n",
    "        for l in range(self.L - 2, -1, -1):\n",
    "            dW_list[l] = np.outer(delta, a_list[l])\n",
    "            db_list[l] = delta\n",
    "            delta = np.dot(self.weights[l].transpose(), delta) * self.g_prime(z_list[l])\n",
    "        return (dW_list, db_list)\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        \"\"\"\n",
    "        Evaluate current model on labeled test data\n",
    "        \"\"\"\n",
    "        ctr = 0\n",
    "        for x, y in test:\n",
    "            yhat = self.forward_prop(x)\n",
    "            ctr += np.argmax(yhat) == np.argmax(y)\n",
    "        return float(ctr) / float(len(test))\n",
    "\n",
    "\n",
    "def sigmoid(z, threshold=20):\n",
    "    z = np.clip(z, -threshold, threshold)\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "\n",
    "def mnist_digit_show(flatimage, outname=None):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    image = np.reshape(flatimage, (-1, 14))\n",
    "\n",
    "    plt.matshow(image, cmap=plt.cm.binary)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if outname:\n",
    "        plt.savefig(outname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART A** Implement *SGD_train, back_prop,forward_prop*. Use the following test cases to verify if the code is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f57fea6be415a9b96f9fffdd56d78c3",
     "grade": true,
     "grade_id": "cell-7632a78793a2588e",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1: Train    0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.195s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PART B **\n",
    "\n",
    "Run the above Network on MNIST Dataset and report the following (feel free to experiment with different learning rates).\n",
    "* Define the hidden layer dimension appropriate for this dataset and report the accuracy for 200 epochs.\n",
    "* Explain the effect of hidden dimension with appropriate plots (check training accuracy and test accuracy). \n",
    "* Explain the effect of number of epochs on MNIST Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input Features:  196\n",
      "Number of Output classes:  10\n",
      "\n",
      "Hidden Layer Dimensions:  25\n",
      "Epoch    1: Train    0.55222, Test    0.49380\n",
      "Epoch   10: Train    0.90956, Test    0.83393\n",
      "Epoch   20: Train    0.93117, Test    0.84874\n",
      "Epoch   30: Train    0.94598, Test    0.87315\n",
      "Epoch   40: Train    0.94958, Test    0.87155\n",
      "Epoch   50: Train    0.95518, Test    0.87715\n",
      "Epoch   60: Train    0.95958, Test    0.87475\n",
      "Epoch   70: Train    0.95998, Test    0.88075\n",
      "Epoch   80: Train    0.95958, Test    0.87955\n",
      "Epoch   90: Train    0.96599, Test    0.88395\n",
      "Epoch  100: Train    0.96719, Test    0.88475\n",
      "Epoch  110: Train    0.96639, Test    0.88355\n",
      "Epoch  120: Train    0.96999, Test    0.88715\n",
      "Epoch  130: Train    0.96879, Test    0.87835\n",
      "Epoch  140: Train    0.96679, Test    0.87955\n",
      "Epoch  150: Train    0.96759, Test    0.88315\n",
      "Epoch  160: Train    0.97279, Test    0.88836\n",
      "Epoch  170: Train    0.97079, Test    0.88595\n",
      "Epoch  180: Train    0.96959, Test    0.89156\n",
      "Epoch  190: Train    0.97079, Test    0.89196\n",
      "Epoch  200: Train    0.96799, Test    0.88475\n"
     ]
    }
   ],
   "source": [
    "location = './data/tinyMNIST.pkl.gz'\n",
    "f = gzip.open(location, 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "train, test = u.load()\n",
    "input_dimensions = len(train[0][0])\n",
    "output_dimensions = len(train[0][1])\n",
    "hidden_layer_dimensions = 25\n",
    "print('Number of Input Features: ', input_dimensions)\n",
    "print('Number of Output classes: ', output_dimensions)\n",
    "print('\\nHidden Layer Dimensions: ', hidden_layer_dimensions)\n",
    "nn = Network([input_dimensions, hidden_layer_dimensions, output_dimensions])\n",
    "nn.SGD_train(train, epochs=200, eta=0.15, lam=0.0001, verbose=True, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2845138055222089, 0.9459783913565426, 0.9655862344937975, 0.9687875150060024, 0.9523809523809523]\n",
      "[0.28731492597038816, 0.8563425370148059, 0.8803521408563425, 0.8863545418167267, 0.8719487795118047]\n"
     ]
    }
   ],
   "source": [
    "acc_train = []\n",
    "acc_test = []\n",
    "hidden_layers = [1, 10, 25, 100, 300]\n",
    "for i in hidden_layers:\n",
    "    nn = Network([input_dimensions, i, output_dimensions])\n",
    "    nn.SGD_train(train, epochs=100, eta=0.15, lam=0.0001, verbose=False, test=test)\n",
    "    acc_train.append(nn.evaluate(train))\n",
    "    acc_test.append(nn.evaluate(test))\n",
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11ebebcf8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Number of Hidden Layers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(hidden_layers, acc_train, label=\"Training Accuracy\")\n",
    "plt.plot(hidden_layers, acc_test, label=\"Testing Accuracy\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot above, increasing the number of hidden layers increases the accuracy (both train and test) very quickly up to about 10 layers. After that point, the accuracy stays very consistent and actually begins a gradual slope down. I didn't run more hidden layers for time sake, but I would expect to continue to see the accuracy decline as the number of hidden layers increases substantially, in part due to the vanishing graident problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of epochs does initially increase both the train and test accuracy. For example, in my model, the jump in test accuracy from epoch 1 to epoch 10 was almost 30%. However, in this case, my test accuracy never got any higher than about 88%, regardless of the number of epochs. It first reached that approximate max at about 100 epochs and there was no further gain from later epochs. So epochs do increase accuracy to a certain point, and then are no longer useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00229a006d4ea5e874a44656862cf7a9",
     "grade": true,
     "grade_id": "cell-f7345360536d4c03",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 Points] Problem 4 - Implement RNN Network to classify whether text is spam or ham \n",
    "---\n",
    "\n",
    "Dataset is obtained from UCI Machine Learning repository consisting of SMS tagged messages (being ham (legitimate) or spam) that have been collected for SMS Spam research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [Keras](https://keras.io/) to implement a classifier (you need to install Keras). Update the below snippet to build a Sequential model with an embedding layer, and an LSTM layer followed by a dense layer. This question allows you to get familiar with popular deep learning toolkits and the solution only has a few lines. In practice, there is no need to reinvent the wheels.\n",
    "\n",
    "\n",
    "Learn more about RNN : https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1861dc0bde6838c86a3ed478308544cb",
     "grade": false,
     "grade_id": "cell-e5faa2bc69b4d8cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    '''\n",
    "    RNN classifier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_x, train_y, test_x, test_y, dict_size=5000,\n",
    "                 example_length=150, embedding_length=32, epoches=5, batch_size=128):\n",
    "        '''\n",
    "        initialize RNN model\n",
    "        :param train_x: training data\n",
    "        :param train_y: training label\n",
    "        :param test_x: test data\n",
    "        :param test_y: test label\n",
    "        :param epoches: number of ephoches to run\n",
    "        :param batch_size: batch size in training\n",
    "        :param embedding_length: size of word embedding\n",
    "        :param example_length: length of examples\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.epoches = epoches\n",
    "        self.example_len = example_length\n",
    "        self.dict_size = dict_size\n",
    "        self.embedding_len = embedding_length\n",
    "\n",
    "        # preprocess training data\n",
    "        tok = Tokenizer(num_words=dict_size)\n",
    "        tok.fit_on_texts(train_x)\n",
    "        sequences = tok.texts_to_sequences(train_x)\n",
    "        self.train_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "        sequences = tok.texts_to_sequences(test_x)\n",
    "        self.test_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "        # TODO: build model with Embedding, LSTM and dense layers.\n",
    "        # refer to Sequence classification with LSTM : https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "        # Documentation for LSTM layer in : https://keras.io/layers/recurrent/#lstm\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.dict_size, output_dim=self.embedding_len))\n",
    "        self.model.add(LSTM(self.embedding_len, dropout = 0.5))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', \n",
    "                           optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        '''\n",
    "        fit in data and train model : refer fit method in https://keras.io/models/model/\n",
    "        make sure you use batchsize and epochs appropriately.\n",
    "        :return:None\n",
    "        '''\n",
    "        # TODO: fit in data to train your model\n",
    "        self.model.fit(self.train_x,self.train_y, self.batch_size, self.epoches, verbose=verbose)\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "\n",
    "        evaluate trained model : Please refer evaluate in https://keras.io/models/model/\n",
    "        :return: [loss,accuracy]\n",
    "        '''\n",
    "        return self.model.evaluate(self.test_x, self.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following functions *init, train and evaluate functions* and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86cd8ba5f6ed607811db54ae4b5634f8",
     "grade": false,
     "grade_id": "cell-b16c4115704765f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "assert keras.__version__ == '2.2.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(location):\n",
    "    return pickle.load(open(location,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4736/4736 [==============================] - 9s 2ms/step - loss: 0.4961 - acc: 0.8480\n",
      "Epoch 2/5\n",
      "4736/4736 [==============================] - 5s 1ms/step - loss: 0.2205 - acc: 0.9039\n",
      "Epoch 3/5\n",
      "4736/4736 [==============================] - 5s 1ms/step - loss: 0.1052 - acc: 0.9768\n",
      "Epoch 4/5\n",
      "4736/4736 [==============================] - 5s 1ms/step - loss: 0.0634 - acc: 0.9869\n",
      "Epoch 5/5\n",
      "4736/4736 [==============================] - 5s 1ms/step - loss: 0.0439 - acc: 0.9911\n",
      "836/836 [==============================] - 2s 2ms/step\n",
      "Accuracy for LSTM:  0.992822966507177\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = load_data('./data/spam_data.pkl')\n",
    "rnn = RNN(train_x, train_y, test_x, test_y, epoches=5)\n",
    "rnn.train(verbose=1)\n",
    "accuracy = rnn.evaluate()\n",
    "print('Accuracy for LSTM: ', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Accuracy after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 5 epochs:  0.992822966507177\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy after 5 epochs: ', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change the embedding length and observe the impact on test and train accuracy.\n",
    "\n",
    "* Explain the impact of embedding length in LSTM Model by providing plots of accuracy vs embedding length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97a255265b4bdabdf5f448cdb5fa9d7d",
     "grade": true,
     "grade_id": "cell-4c9a9edc927e69ee",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836/836 [==============================] - 2s 2ms/step\n",
      "836/836 [==============================] - 2s 2ms/step\n",
      "836/836 [==============================] - 2s 2ms/step\n",
      "836/836 [==============================] - 2s 3ms/step\n",
      "836/836 [==============================] - 2s 3ms/step\n",
      "836/836 [==============================] - 2s 3ms/step\n",
      "836/836 [==============================] - 3s 3ms/step\n",
      "[0.8708133971291866, 0.8708133971291866, 0.8755980861244019, 0.9617224880382775, 0.9844497607655502, 0.9916267942583732, 0.9964114832535885]\n"
     ]
    }
   ],
   "source": [
    "embedding_lengths = [2**x for x in range(0,7)]\n",
    "accuracy_list = []\n",
    "for l in embedding_lengths:\n",
    "    rnn2 = RNN(train_x, train_y, test_x, test_y, epoches=5, embedding_length=l)\n",
    "    rnn2.train(verbose=0)\n",
    "    accuracy_list.append(rnn2.evaluate()[1])\n",
    "print(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb38869ac8>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Embedding Length\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(embedding_lengths, accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, as the embedding length increases, the accuracy increases. There is a sharp jump in accuracy around embedding_len = 4, and then the accuracy begins to level out. This means that a higher dimensionality of LTSM output can provided greater accuracy, but at a cosr of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
